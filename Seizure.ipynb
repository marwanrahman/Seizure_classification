{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 221
    },
    "colab_type": "code",
    "id": "H7--4JADIrfV",
    "outputId": "7439c230-5463-4894-d609-bf38b6759b84"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "l06Hio9vIgC3"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Conv3D, MaxPool3D, Flatten, Dense\n",
    "from tensorflow.keras.layers import Dropout, Input, BatchNormalization\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "# from plotly.offline import iplot, init_notebook_mode\n",
    "from tensorflow.keras.losses import categorical_crossentropy\n",
    "from tensorflow.keras.optimizers import Adadelta\n",
    "import tensorflow.keras.metrics\n",
    "# import plotly.graph_objs as go\n",
    "# from matplotlib.pyplot import cm\n",
    "from tensorflow.keras.models import Model\n",
    "import numpy as np\n",
    "# import keras\n",
    "import h5py\n",
    "import pandas as pd\n",
    "import re\n",
    "# import glob\n",
    "# from google.colab import drive\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "from scipy.io import loadmat\n",
    "from pyts.image import GramianAngularField\n",
    "from pyts.image import RecurrencePlot\n",
    "import sys\n",
    "from os import walk\n",
    "\n",
    "import os\n",
    "from google.cloud import storage\n",
    "import gcsfs\n",
    "from os import environ\n",
    "from imblearn.over_sampling import (SMOTE, BorderlineSMOTE, SVMSMOTE, SMOTENC, KMeansSMOTE)\n",
    "from imblearn.over_sampling import ADASYN\n",
    "from imblearn.under_sampling import NearMiss \n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_layer = Input((108, 108, 16,1))\n",
    "\n",
    "## convolutional layers\n",
    "conv_layer1 = Conv3D(filters=64, kernel_size=(3, 3, 3), strides=(2, 2, 2),padding='same')(input_layer)\n",
    "leaky_layer1 = tf.keras.layers.LeakyReLU()(conv_layer1)\n",
    "\n",
    "# conv_layer1 = (conv_layer1)(leaky_layer1)\n",
    "\n",
    "## add max pooling to obtain the most imformatic features\n",
    "pooling_layer1 = MaxPool3D(pool_size=(2, 2, 2), strides =(2,2,2))(leaky_layer1)\n",
    "\n",
    "\n",
    "conv_layer2 = Conv3D(filters=128, kernel_size=(3, 3, 3), strides=(1, 1, 1),padding='same')(pooling_layer1)\n",
    "leaky_layer2 = tf.keras.layers.LeakyReLU()(conv_layer2)\n",
    "\n",
    "## add max pooling to obtain the most imformatic features\n",
    "pooling_layer2 = MaxPool3D(pool_size=(2, 2, 2), strides = (1,2,2))(leaky_layer2)\n",
    "\n",
    "conv_layer3 = Conv3D(filters=256, kernel_size=(3,3,3), strides=(1,1,1),padding='same')(pooling_layer2)\n",
    "leaky_layer3 = tf.keras.layers.LeakyReLU()(conv_layer3)\n",
    "\n",
    "conv_layer4 = Conv3D(filters=256, kernel_size=(3,3,3), strides=(1,1,1),padding='same')(leaky_layer3)\n",
    "leaky_layer4 = tf.keras.layers.LeakyReLU()(conv_layer4)\n",
    "\n",
    "pooling_layer3 = MaxPool3D(pool_size=(2, 2, 2), strides = (1,2,2))(leaky_layer4)\n",
    "\n",
    "# flatten before passing to fully connected layer \n",
    "pooling_layer3 = BatchNormalization()(pooling_layer3)\n",
    "\n",
    "flatten_layer=Flatten()(pooling_layer3)\n",
    "\n",
    "\n",
    "dense_layer1 = Dense(units=128)(flatten_layer)\n",
    "dense_layer1 = Dropout(0.5)(dense_layer1)\n",
    "\n",
    "dense_layer2 = Dense(units=64)(dense_layer1)\n",
    "dense_layer2 = Dropout(0.5)(dense_layer2)\n",
    "\n",
    "output_layer = Dense(units=2, activation='softmax')(dense_layer2)\n",
    "model = Model(inputs=input_layer, outputs=output_layer)\n",
    "\n",
    "model.compile(loss=categorical_crossentropy, optimizer=tf.keras.optimizers.Adam(learning_rate=0.01, beta_1=0.9, beta_2=0.999), metrics=['accuracy'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "G:\\Seizure_Data\\credentials\\seizure-prediction123-593d1ad578b8.json\n"
     ]
    }
   ],
   "source": [
    "# Credentials should be set on terminal prior to running on gcp\n",
    "# os.environ[\"GOOGLE_APPLICATION_CREDENTIALS\"] = r\"G:\\Seizure_Data\\credentials\\seizure-prediction123-593d1ad578b8.json\"\n",
    "print(os.environ.get('GOOGLE_APPLICATION_CREDENTIALS'))\n",
    "client=storage.Client()\n",
    "os.environ[\"GCP_BUCKET_NAME\"] = \"marwansrikarharris\"\n",
    "bucket_name = os.environ.get(\"GCP_BUCKET_NAME\")\n",
    "bucket = client.get_bucket(bucket_name)\n",
    "files = bucket.list_blobs(prefix=\"Data/\")\n",
    "fileList = [file.name for file in files if '.' in file.name]\n",
    "fs=gcsfs.GCSFileSystem(project='seizure-prediction123')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "jqYCuXY90M2i"
   },
   "source": [
    "Assemble data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assemble list of file names for each dog\n",
    "Dog_1_files = []\n",
    "Dog_2_files = []\n",
    "Dog_3_files = []\n",
    "Dog_4_files = []\n",
    "Dog_5_files = []\n",
    "\n",
    "for idx, filename in enumerate(fileList):\n",
    "    Dog_1_search = re.search(r\"Dog_1\", filename)\n",
    "    Dog_2_search = re.search(r\"Dog_2\", filename)\n",
    "    Dog_3_search = re.search(r\"Dog_3\", filename)\n",
    "    Dog_4_search = re.search(r\"Dog_4\", filename)\n",
    "    Dog_5_search = re.search(r\"Dog_5\", filename)\n",
    "\n",
    "    if Dog_1_search:\n",
    "        Dog_1_files.append(filename)\n",
    "    elif Dog_2_search:\n",
    "        Dog_2_files.append(filename)\n",
    "    elif Dog_3_search:\n",
    "        Dog_3_files.append(filename)\n",
    "    elif Dog_3_search:\n",
    "        Dog_3_files.append(filename)\n",
    "    elif Dog_4_search:\n",
    "        Dog_4_files.append(filename)\n",
    "    elif Dog_5_search:\n",
    "        Dog_5_files.append(filename)\n",
    "\n",
    "# list containing the indices of interictal and preictal data end        \n",
    "master_indices =np.zeros((5 ,2))\n",
    "master_indices[0][0 ] =479\n",
    "master_indices[0][1 ] =503\n",
    "master_indices[1][0 ] =499\n",
    "master_indices[1][1 ] =541\n",
    "master_indices[2][0 ] =1439\n",
    "master_indices[2][1 ] =1511\n",
    "master_indices[3][0 ] =803\n",
    "master_indices[3][1 ] =900\n",
    "master_indices[4][0 ] =449\n",
    "master_indices[4][1 ] =479\n",
    "\n",
    "master_indices[0][0]=803\n",
    "master_indices[0][1]=900\n",
    "\n",
    "all_dogs =[]\n",
    "all_dogs.append(Dog_1_files)\n",
    "all_dogs.append(Dog_2_files)\n",
    "all_dogs.append(Dog_3_files)\n",
    "all_dogs.append(Dog_4_files)\n",
    "all_dogs.append(Dog_5_files)\n",
    "\n",
    "all_dogs =np.array(all_dogs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-vfM2ZxhJuSw"
   },
   "outputs": [],
   "source": [
    "# get mariables from .mat structs\n",
    "def get_variables(x,path,searched_string):\n",
    "    searched_string=searched_string.group(0)\n",
    "    searched_string=re.sub(r\"_[0]+(?=[1-9])\",\"_\",searched_string)\n",
    "    time_series_data=x[searched_string][0][0][0]\n",
    "    data_length_sec=x[searched_string][0][0][1]\n",
    "    sampling_frequency=data_length_sec=x[searched_string][0][0][2]\n",
    "    channels=data_length_sec=x[searched_string][0][0][3]\n",
    "    assemble_matrix(time_series_data,path)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "i2OzLLe0JzPZ"
   },
   "outputs": [],
   "source": [
    "#represent time series data as a Gramiam plot\n",
    "def convert_to_Gramiam(channel_data):\n",
    "    transformer=RecurrencePlot()\n",
    "    X_new = transformer.transform(channel_data)\n",
    "    input_size=3996\n",
    "    output_size=111\n",
    "    bin_size = input_size // output_size\n",
    "    X_new = X_new.reshape((1,output_size, bin_size, \n",
    "                                   output_size, bin_size,)).max(4).max(2)\n",
    "\n",
    "# Change datatype to lower memory usage\n",
    "    X_new=X_new.astype(np.float16)\n",
    "    return X_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "uBC5SVG1J1Jx"
   },
   "outputs": [],
   "source": [
    "# Split data into 10 second long segments\n",
    "def assemble_matrix(time_series_data,path):\n",
    "    time_series_section=time_series_data[:,3:239763]\n",
    "    equal_partitions=np.hsplit(time_series_section,60)\n",
    "    image_array=get_image_matrix(equal_partitions)\n",
    "    assemble_y(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "vrlrHvm1J2q9"
   },
   "outputs": [],
   "source": [
    "# convert obtain an image for each image\n",
    "def get_image_matrix(equal_partitions):]\n",
    "    for idx, segment in enumerate(equal_partitions):\n",
    "        image_matrix=[]\n",
    "        for index, single_channel in enumerate(segment,start=0):\n",
    "            single_channel=np.reshape(single_channel,(1,len(single_channel)))\n",
    "            image_matrix.append(convert_to_Gramiam(single_channel))\n",
    "        master_array.append(create_3D_image_array(image_matrix))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "P0h3gRQvJ6QC"
   },
   "outputs": [],
   "source": [
    "# compile array of gramiam images for each channel\n",
    "def create_3D_image_array(image_matrix):\n",
    "    image_1=image_matrix[0][0]\n",
    "    image_2=image_matrix[1][0]\n",
    "    image_3=image_matrix[2][0]\n",
    "    image_4=image_matrix[3][0]\n",
    "    image_5=image_matrix[4][0]\n",
    "    image_6=image_matrix[5][0]\n",
    "    image_7=image_matrix[6][0]\n",
    "    image_8=image_matrix[7][0]\n",
    "    image_9=image_matrix[8][0]\n",
    "    image_10=image_matrix[9][0]\n",
    "    image_11=image_matrix[10][0]\n",
    "    image_12=image_matrix[11][0]\n",
    "    image_13=image_matrix[12][0]\n",
    "    image_14=image_matrix[13][0]\n",
    "    image_15=image_matrix[14][0]\n",
    "    image_16=image_matrix[15][0]\n",
    "\n",
    "#     image_array_3D=np.dstack([image_1,image_2,image_3,image_4,image_5,image_6,image_7,image_8,image_9,\n",
    "#                               image_10,image_11,image_12,image_13,image_14,image_15,image_16])\n",
    "\n",
    "    image_array_3D=np.dstack([image_1,image_2,image_3,image_4,image_5,image_6,image_7,image_8,image_9,\n",
    "                              image_10,image_11,image_12,image_13,image_14,image_15])\n",
    "\n",
    "    return image_array_3D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "WKzLooQMJ8xZ"
   },
   "outputs": [],
   "source": [
    "def add_to_master(list_of_3D_arrays):\n",
    "    sample_3D_array=list_of_3D_arrays\n",
    "    master_array.append(list_of_3D_arrays)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "k0yf93e_J-SM"
   },
   "outputs": [],
   "source": [
    "# Create y labels\n",
    "def assemble_y(path):\n",
    "    match=re.search('preictal',path)\n",
    "    if match:\n",
    "        for i in range(60):\n",
    "            master_Y.append([0,1])\n",
    "    else:\n",
    "        for i in range(60):\n",
    "            master_Y.append([1,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "FNyDkMBKKBB-"
   },
   "outputs": [],
   "source": [
    "def list_to_array():\n",
    "    all_X=np.array(master_array)\n",
    "    all_Y=np.array(master_Y)\n",
    "    return all_X,all_Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For each training iteration, obtain two preictal files\n",
    "def get_preictal_data(dog_files,interictal_end,preictal_end,curr_iteration):\n",
    "    print(\"GETTING PREICTAL DATA\")\n",
    "    values_to_pass=curr_iteration*2    \n",
    "    sequence_start=interictal_end+1+values_to_pass\n",
    "    sequence_end=sequence_start+1\n",
    "    \n",
    "    \n",
    "    file_names=[]\n",
    "    file_names.append(dog_files[int(sequence_start)])\n",
    "    file_names.append(dog_files[int(sequence_end)])\n",
    "    \n",
    "    for file in file_names:\n",
    "        file_dir=bucket_name+r\"/\"+(file)\n",
    "        with fs.open(file_dir,'rb') as f:\n",
    "            matching_string=r\"(?<=[0-9]_)[\\S]+(?=.mat)\"\n",
    "            searched_string=re.search(matching_string,file)\n",
    "            data_file=file\n",
    "            print(\"Filename: {} \" .format(data_file))\n",
    "            x=loadmat(f)\n",
    "            get_variables(x,data_file,searched_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hashmap to return random indices of interical data in 2-D matrix \n",
    "def get_indices(runs, end_idx):\n",
    "    runs = int(runs)\n",
    "    dict = {}\n",
    "    cur_id = 0\n",
    "    interictal_index = np.zeros([runs,8])\n",
    "    for x in range(runs):\n",
    "        for y in range(8):\n",
    "            exists = True\n",
    "            temp = 0\n",
    "            while(exists):\n",
    "                temp = random.randint(1, end_idx)\n",
    "                if not temp in dict.values():\n",
    "                    exists = False\n",
    "            interictal_index[x,y] = temp\n",
    "            dict[cur_id] = temp\n",
    "            cur_id += 1\n",
    "    return interictal_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main function iterate through file names and train model\n",
    "def run():\n",
    "# class imbalance techinques\n",
    "    sm = SMOTE(random_state=42, sampling_strategy=1)\n",
    "    rus = RandomUnderSampler(random_state=42, sampling_strategy=0.5)\n",
    "    \n",
    "    for idx, dog_files in enumerate(all_dogs):\n",
    "        \n",
    "# For each iteration, get 8 random interical files. \n",
    "        inter_pre_indices = master_indices[idx]\n",
    "        interictal_end = inter_pre_indices[0]\n",
    "        run_number = (inter_pre_indices[1] - interictal_end) / 2\n",
    "        dog_indices = get_indices(run_number, interictal_end)\n",
    "\n",
    "        for row_num, row in enumerate(dog_indices):\n",
    "            master_array = []\n",
    "            master_Y = []\n",
    "            for i, index in enumerate(row):\n",
    "                file_name = dog_files[int(index)]\n",
    "                file_dir = bucket_name + r\"/\" + (file_name)\n",
    "                with fs.open(file_dir, 'rb') as f:\n",
    "                    matching_string = r\"(?<=[0-9]_)[\\S]+(?=.mat)\"\n",
    "                    searched_string = re.search(matching_string, file_name)\n",
    "                    data_file = file_name\n",
    "                    print(\"Filename: {} \".format(data_file))\n",
    "                    print(\"ClassName: {} \".format(searched_string.group(0)))\n",
    "                    x = loadmat(f)\n",
    "                    get_variables(x, data_file, searched_string, master_array, master_Y)\n",
    "            get_preictal_data(dog_files, interictal_end, inter_pre_indices[1], row_num, master_array, master_Y)\n",
    "            print(\"ONE ITERATION COMPLETE\")\n",
    "            all_X, all_Y = list_to_array(master_array, master_Y)\n",
    "            all_X = all_X.reshape(len(all_X), 174960)\n",
    "            \n",
    "# Implement imbalance algorithms\n",
    "            all_X, all_Y = rus.fit_resample(all_X, all_Y)\n",
    "            new_y = []\n",
    "            for k, val in enumerate(all_Y):\n",
    "                if val[0] == 1:\n",
    "                    new_y.append([0, 1])\n",
    "                else:\n",
    "                    new_y.append([1, 0])\n",
    "            new_y = np.array(new_y)\n",
    "            all_Y = new_y\n",
    "\n",
    "            new_y = None\n",
    "            new_y = []\n",
    "            all_X, all_Y = sm.fit_resample(all_X, all_Y)\n",
    "            for k, val in enumerate(all_Y):\n",
    "                if val[0] == 1:\n",
    "                    new_y.append([0, 1])\n",
    "                else:\n",
    "                    new_y.append([1, 0])\n",
    "\n",
    "            new_y = np.array(new_y)\n",
    "            \n",
    "# Train model on batch\n",
    "            all_X = all_X.reshape(len(all_X), 108, 108, 15, 1)\n",
    "            metrics=model.train_on_batch(x=all_X, y=new_y)\n",
    "            print(metrics)\n",
    "            \n",
    "#Reset variables to optimize memory usage\n",
    "            all_X = None\n",
    "            all_Y = None\n",
    "            x_train = None\n",
    "            y_train = None\n",
    "            master_array = None\n",
    "            master_Y = None\n",
    "            master_array = []\n",
    "            master_Y = []\n",
    "            new_y = None\n",
    "            gc.collect()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "master_array=[]\n",
    "master_Y=[]\n",
    "run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If running on GCP, model automatically saves to bucket \n",
    "# model.save(r'C:\\Users\\harri\\Desktop\\CorTech\\TeamAlpha\\Saved_Model\\savedModel3.h5')"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "machine_shape": "hm",
   "name": "Seizure.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
